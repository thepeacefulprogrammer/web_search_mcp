# Project Rules for Web Search MCP Server

## Automatic Rule Fetching (Always Applied)

When the user's query mentions specific keywords or contexts, automatically fetch and follow the detailed rules:

1. **PRD Creation**: If the query mentions "PRD", "requirements", "product requirements", or "specification" → ALWAYS fetch the `create-prd` rule first and follow it
2. **Task Generation**: If the query mentions "tasks", "task breakdown", "implementation plan", or "steps" → ALWAYS fetch the `generate-tasks` rule first and follow it
3. **Task Execution**: If working with existing task lists or implementing tasks → Follow the task list management rules below

## Task List Management (Always Applied)

Guidelines for managing task lists in markdown files to track progress on completing a PRD

### Task Implementation
- **One sub-task at a time:** Do **NOT** start the next sub-task until you ask the user for permission and they say "yes" or "y"
- **Completion protocol:**
  1. When you finish a **sub-task**, run the associated unit tests before you mark it as completed by changing `[ ]` to `[x]`. Note, you do not need to run the full test suite until we finish all tasks under a parent task.
  2. If **all** subtasks underneath a parent task are now `[x]`, also mark the **parent task** as completed.
  3. If **parent task** is complete, run the full test suite via ```python -m pytest tests/unit/ -n auto -q --tb=line```
- **Test-Driven Development (TDD):** For each sub-task, start by writing unit test(s) that capture the expected behavior. Run the tests to confirm they fail. Then implement the code changes to make the tests pass. Only after the tests pass should you apply the completion protocol.
- **Test suite enforcement:** After implementing a sub-task, run the project's test suite (e.g., `pytest`). Do **not** mark the sub-task as complete until all tests pass. If tests fail, iterate on the code changes for that sub-task until the suite passes, then apply the completion protocol.

### AI Instructions for Task Lists
When working with task lists, the AI must:
- **Adopt Test-Driven Development:** Before starting work on a sub-task, write the corresponding unit test(s), run them to verify failure, then implement code to satisfy the tests, and ensure they pass before proceeding.
1. Regularly update the task list file after finishing any significant work.
2. Follow the completion protocol:
   - Mark each finished **sub-task** `[x]` only after verifying the test suite passes.
   - Mark the **parent task** `[x]` once **all** its subtasks are `[x]` and the test suite is green.
3. Add newly discovered tasks.
4. Keep "Relevant Files" accurate and up to date.
5. Before starting work, check which sub-task is next.
6. After implementing a sub-task, update the file and then pause for user approval.
7. After marking a sub-task complete, immediately run the full test suite; if any tests fail, revert the completion mark, fix the code for that sub-task, and re-run tests until they pass before re-marking as complete.

## File Organization Rules (Always Applied)

### Root Directory Policy
**NEVER** place the following file types directly in the project root:
- Python scripts (`.py` files) except for essential entry points
- Debug/testing scripts
- Temporary utility scripts
- Example/demo files
- Tool scripts

### Required Directory Structure
- `/src/` - All main application source code, core modules and packages, production code only
- `/tests/` - Unit tests, integration tests, test fixtures, all files with `test_` prefix or `_test` suffix
- `/examples/` - Demo scripts, sample applications, tutorial code, manual test applications
- `/tools/` - Debug scripts, build tools, development utilities, CI/CD scripts
- `/tasks/` - Task lists, project documentation, implementation tracking
- `/docs/` - API documentation, user guides, technical specifications (if needed)

### Root Directory - Allowed Files Only
- `README.md` - Project overview
- `requirements.txt` - Dependencies
- `pytest.ini` - Test configuration
- `.env*` - Environment files
- `.gitignore` - Git configuration
- `setup.py` / `pyproject.toml` - Package configuration (if needed)

### Enforcement Rules
1. **Before creating any `.py` file**, ask yourself:
   - Is this a core source file? → `/src/`
   - Is this a test? → `/tests/`
   - Is this an example/demo? → `/examples/`
   - Is this a debug/utility tool? → `/tools/`

2. **Never place temporary files at root** - Use appropriate subdirectories and clean up temporary files when done

3. **File naming conventions**:
   - Debug scripts: `tools/debug_*.py`
   - Example files: `examples/example_*.py` or `examples/*_demo.py`
   - Test files: `tests/test_*.py`
   - Utility tools: `tools/tool_*.py` or `tools/*_tool.py`

## Project Context
This is a web search MCP (Model Context Protocol) server project being converted from an MCP scaffolding template. The goal is to provide web search capabilities to AI assistants like Claude through a standardized MCP interface. 